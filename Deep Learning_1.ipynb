{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e49e64",
   "metadata": {},
   "source": [
    "# 12.1 딥러닝 소개\n",
    "\n",
    "인간의 각 뉴런은 시냅스로 연결되는데, 자극의 크기가 특정값 이상이면 다른 뉴런에 전달하고, 특정값 이하라면 다른 뉴런에 전달하지 않는다. 이때, 자극을 전달하는 기준인 특정값을 임계치라고 한다. 그래서 인간은 임계치를 넘긴 자극을 신호로 전달받고 그에 해당하는 반응을 하게 되는 것이다.<br>\n",
    "\n",
    "<p style = \"font-size : 15px ; font-weight : bold\"> 1. 인공 신경망(Artificial Neural Network) </p>\n",
    "\n",
    "   * 생명 과학 분야의 아이디어를 머신러닝에 도입한 것<br>\n",
    " \n",
    "<p style = \"font-size : 15px ; font-weight : bold\"> 2. 딥러닝(Deep Learning) </p>\n",
    "\n",
    "   * 신경망을 기반으로 학습하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ef20e",
   "metadata": {},
   "source": [
    "# 12.2 퍼셉트론, 딥러닝의 기본\n",
    "\n",
    "### 12.2.1 퍼셉트론의 개념\n",
    "\n",
    "<p style = \"font-size : 15px ; font-weight : bold\"> 1. 퍼셉트론(Perceptron) </p>\n",
    "\n",
    " * 신경망의 최소 단위\n",
    " * n개의 입력값(Input)을 합쳐 입력값 벡터 **x**로 표현<br>\n",
    " * n개의 가중치 값을 합쳐 가중치 벡터 **w**로 표현<br>\n",
    " * 입력값 벡터와 가중치 벡터의 내적값은 출력값(Output) **z**로 표현<br>\n",
    " ![perceptron](https://blog.kakaocdn.net/dn/biE5T3/btqB3qPkNmb/ZfUn6Xo2Np6kJutwdKQO6k/img.png)<br>\n",
    " \n",
    " * 가중합(Weight Sum) : 입력 데이터에 가중치를 곱한 후 더한 값<br>\n",
    " * 활성화 함수(Activation Function) : 출력값을 결정하는 함수<br>\n",
    "  * 계단 함수, 시그모이드(Sigmoid), 렐루(ReLU), 리키 렐루(Leaky ReLU) 등<br>\n",
    " * 가중합은 활성화 함수를 거치면서 최종 출력값 **y**를 반환<br>\n",
    "  * 가중합과 활성화 함수는 하나의 노드(Node)라고 생각<br><br>\n",
    "  \n",
    "### 12.2.2 퍼셉트론으로 분류하기\n",
    "\n",
    " * 입력값(Input) : 이미지 데이터<br>\n",
    "\n",
    " 1. 이미지 픽셀값에 기반한 행렬을 신경망에 넣기 위해 벡터로 변환\n",
    "  * 행렬을 벡터로 변환 = 행렬 원소를 길게 늘어뜨려 벡터의 형태로 만드는 것<br>\n",
    " 2. 입력 데이터와 가중치 벡터, 편향을 이용하여 가중합을 구함<br>\n",
    " 3. 활성화 함수를 통해 점수를 출력<br>\n",
    " 4. 점수를 비교한 후 최종적으로 점수가 더 높은 것으로 분류<br><br>\n",
    "\n",
    "### 12.2.3 퍼셉트론 실습<br>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 입력층\n",
    "input_data = np.array([[2, 3], [5, 1]])\n",
    "x = input_data.reshape(-1)\n",
    "\n",
    "#가중치 및 편향\n",
    "w1 = np.array([2, 1, -3, 3])\n",
    "w2 = np.array([1, -3, 1, 3])\n",
    "b1 = 3\n",
    "b2 = 3\n",
    "\n",
    "# 가중합\n",
    "W = np.array([w1, w2])\n",
    "b = np.array([b1, b2])\n",
    "weight_sum = np.dot(W, x) + b\n",
    "\n",
    "# 출력층\n",
    "res = 1 / (1 + np.exp(-weight_sum))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb4619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬을 다루기 위한 라이브러리\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297b7fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [5 1]]\n",
      "[2 3 5 1]\n"
     ]
    }
   ],
   "source": [
    "# 입력층\n",
    "input_data = np.array([[2, 3], [5, 1]]) # array method를 이용하여 행렬 데이터 생성\n",
    "print(input_data) # 2행 2열\n",
    "\n",
    "x = input_data.reshape(-1) # 기존 행렬을 벡터로 바꾸기 위해 reshape 함수를 사용하고 인자 값으로 -1 입력\n",
    "print(x) # 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885d6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#가중치 및 편향\n",
    "w1 = np.array([2, 1, -3, 3]) # 가중치 벡터 1\n",
    "w2 = np.array([1, -3, 1, 3]) # 가중치 벡터 2\n",
    "b1 = 3 # 편향 1\n",
    "b2 = 3 # 편향 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e17e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  1 -3  3]\n",
      " [ 1 -3  1  3]]\n",
      "[3 3]\n",
      "[-2  4]\n"
     ]
    }
   ],
   "source": [
    "# 가중합\n",
    "W = np.array([w1, w2]) # 가중치 벡터 1, 2를 합쳐 가중치 행렬 W 설정\n",
    "print(W) # 2행 4열\n",
    "\n",
    "b = np.array([b1, b2]) # 편향 1, 2를 합쳐 편향 벡터 구함\n",
    "print(b)\n",
    "\n",
    "weight_sum = np.dot(W, x) + b # 가중합을 구하기 위해 np.dot을 이용하여 행렬곱 계산\n",
    "print(weight_sum) # 가중합 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757e170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11920292 0.98201379]\n"
     ]
    }
   ],
   "source": [
    "# 출력층\n",
    "res = 1 / (1 + np.exp(-weight_sum)) # Sigmoid 함수를 활성화 함수로 사용\n",
    "print(res) # 최종 점수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4245992",
   "metadata": {},
   "source": [
    "# 12.3 인공 신경망으로 하는 딥러닝\n",
    "\n",
    "### 12.3.1 신경망의 개념\n",
    "\n",
    "* 다층 퍼셉트론(Multi-Layer Perceptron) : 퍼셉트론의 층이 여러 개<br>\n",
    "* 기존의 데이터 공간을 변형함으로써 기존의 하나의 퍼셉트론으로는 해결할 수 없었던 문제를 해결할 수 있게 되는 것<br>\n",
    "  \n",
    "<p style = \"font-size : 15px ; font-weight : bold\"> 1. 인공 신경망(Artificial Neural Network) </p>\n",
    "\n",
    "* **신경망(Neural Network)**\n",
    " * 다수의 뉴런을 사용해 만든 것<br>\n",
    " ![neural](https://mblogthumb-phinf.pstatic.net/MjAxNzExMTdfMTMy/MDAxNTEwOTA3OTg2MjMy.oPFQMQQh_8p2McfTuxwbtju0IJj8zoVYi_ExH74FH4Ig.W9_cpin9YT53sATdFPyz3n9liWZi_Wj3sUxXORjQrW4g.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)<br>\n",
    " \n",
    " **Example. 신경망 예제 - 붓꽃 데이터를 이용하여 꽃의 종류(클래스)를 분류**<br>\n",
    "  1) 입력층에 붓꽃 데이터의 피처(꽃받침 길이, 넓이, 꽃잎 길이, 넓이, ...) 입력<br>\n",
    "  2) 해당 데이터들은 은닉층을 거침<br>\n",
    "  3) 출력층을 통해 출력<br>\n",
    "     * 출력층의 노드 개수는 분류하려는 클래스의 수와 같음<br><br>\n",
    "   \n",
    "* **스코어(Score)** : 출력층의 각 노드가 나타내는 수<br>\n",
    " * 스코어가 높을수록 해당 클래스에 속할 확률이 높다는 뜻<br>\n",
    " * 최종적으로 스코어가 가장 높은 클래스를 선정<br>\n",
    "  \n",
    "<p style = \"font-size : 15px ; font-weight : bold\"> 2. 신경망의 합성 함수 형태 </p>\n",
    "\n",
    "* $f(x) = f_3(f_2(f_1(x)))$\n",
    "  * $f_1$ : 첫 번째 신경망 층\n",
    "  * $f_2$ : 두 번째 신경망 층\n",
    "  * $f_3$ : 세 번째 신경망 층<br><br>\n",
    " \n",
    "* 입력층 → 은닉층 : Vector-to-Vector<br>\n",
    "* 딥러닝(Deep Learning)에서의 함수 : Vector-to-Scalar<br><br>\n",
    " \n",
    "### 12.3.2 오차 역전파\n",
    "\n",
    "* **오차 역전파(Back Propagation)** : 다층 퍼셉트론에서 최적값을 찾아가는 과정<br>\n",
    "* 순전파(Forward Propagation) : 입력층 → 은닉층 → 출력층 순서대로 흘러가는 것\n",
    "* 역전파(Backward Propagation) : 출력층 → 은닉층 → 입력층 순서대로 반대로 거슬러 올라가는 것<br>\n",
    "\n",
    "1. **가중치 초기화**<br><br>\n",
    " \n",
    "2. **순전파(Forward Propagation)를 통한 출력값 계산**<br><br>\n",
    "   1. 입력층 → 은닉층<br>\n",
    "     * $\\mathrm{W}_1^{T}x + b_1 = g$<br>\n",
    "       * $\\mathrm{W}_1^{T}x$ : 가중치 × 입력값<br>\n",
    "       * $b_1$ : 편향값 1<br>\n",
    "       * $g$ : 입력층 → 은닉층 결과값<br>\n",
    "     \n",
    "   2. 은닉층 → 출력층<br>\n",
    "     * $\\mathrm{W}_2^{T}h + b_2 = z$<br>\n",
    "       * $\\mathrm{W}_2^{T}h$ : 가중치 × 활성화 함수를 거친 은닉층 결과값\n",
    "       * $b_2$ : 편향값 2\n",
    "       * $z$ : 은닉층 → 출력층 결과값<br>\n",
    "  \n",
    "   3. 최종 결과값 $y$<br><br>\n",
    "  \n",
    "3. **비용 함수(Cost Function) 정의 및 1차 미분식 구하기**<br>\n",
    " * 오차 제곱합 : 신경망 모형을 이용해 구한 계산값 $y$와 실제값 $t$의 차를 제곱하고 모두 더한 것<br>\n",
    " * $C = {1\\over2}[(y_1 - t_1)^2 + (y_2 - t_2)^2]$<br>\n",
    "   * ${dC\\over dy_1} = y_1 - t_1$<br>\n",
    "   * ${dC\\over dy_2} = y_2 - t_2$<br><br>\n",
    "   \n",
    "4. **역전파(Backward Propagation)를 통한 1차 미분값 구하기**<br>\n",
    "   1. 출력층 → 은닉층<br>\n",
    "     * $\\mathrm{W}_2^{T}h + b_2 = z$<br>\n",
    "       * ${dz\\over dW_2} = h$<br>\n",
    "       * ${dz\\over db_2} = 1$<br>\n",
    "     * $\\begin{pmatrix} w_41 & w_51 \\\\ w_42 & w_52 \\end{pmatrix}\\begin{pmatrix} h_1 \\\\ h_2 \\end{pmatrix} + \\begin{pmatrix} b_2 \\\\ b_2 \\end{pmatrix} = \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}$<br>\n",
    "       * $w_{41}h_1 + w_{51}h_2 + b_2 = z_1$<br>\n",
    "       * $w_{42}h_1 + w_{52}h_2 + b_2 = z_2$<br>\n",
    "    \n",
    "   2. 은닉층 → 입력층<br>\n",
    "     * $\\mathrm{W}_1^{T}x + b_1 = g$<br>\n",
    "       * ${dg\\over dW_1} = x$<br>\n",
    "       * ${dg\\over db_1} = 1$<br>\n",
    "     * $\\begin{pmatrix} w_11 & w_21 & w_31 \\\\ w_12 & w_22 & w_32 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} + \\begin{pmatrix} b_1 \\\\ b_1 \\end{pmatrix} = \\begin{pmatrix} g_1 \\\\ g_2 \\end{pmatrix}$<br>\n",
    "       * $w_{11}x_1 + w_{21}x_2 + w_{31}x_3 + b_1 = g_1$<br>\n",
    "       * $w_{12}x_1 + w_{22}x_2 + w_{31}x_3 + b_1 = g_2$<br><br>\n",
    "   \n",
    "5. **파라미터(Parameter) 업데이트**<br>\n",
    " * 가중치를 업데이트하는 과정<br>\n",
    " * $w_{nm} → w_{nm} - \\eta {dC\\over dw_{nm}}$<br>\n",
    " * $b_n → b_n - \\eta {dC\\over db_n}$\n",
    "    * $\\eta$ : 학습률<br><br>\n",
    " \n",
    "6. **과정 반복(2 ~ 6)**<br><br>\n",
    "\n",
    "### 12.3.3 활성화 함수\n",
    "\n",
    "1. **계단 함수(Step Function)**<br>\n",
    " * $\\phi(x) = \\begin{cases} 0, & \\mbox{x }\\le\\mbox{0} \\\\ 1, & \\mbox{x }>\\mbox{0} \\end{cases}$<br>\n",
    " * 입력값이 0 이하일 경우에는 0을 출력, 0을 초과할 때만 1을 출력<br>\n",
    " * 계단 함수의 출력값은 0 또는 1<br>\n",
    " ![step](https://blog.kakaocdn.net/dn/bVAgGW/btqv3sTf0pi/jdzlln0jIGmLnz6LVUl0FK/img.png)<br>\n",
    " \n",
    " * 장점 : 사용하기 간단함<br>\n",
    " * 단점 : 미분이 가능하지 않음<br><br>\n",
    " \n",
    "2. **부호 함수(Sign Function)**<br>\n",
    " * $\\phi(x) = \\begin{cases} 1, & \\mbox{x }>\\mbox{0} \\\\ 0, & \\mbox{x }=\\mbox{0} \\\\ -1, & \\mbox{x }<\\mbox{0} \\end{cases}$<br>\n",
    " ![sign](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Signum_function.svg/1200px-Signum_function.svg.png)<br>\n",
    " \n",
    " * -1, 0, 1 값을 출력<br><br>\n",
    " \n",
    "3. **시그모이드 함수(Sigmoid Function)**<br>\n",
    " * $\\phi(x) = {1 \\over {1 + \\exp(-x)}}$<br>\n",
    " ![sigmoid](https://images.velog.io/images/arittung/post/6765d80c-34ec-4360-94f9-79797b3dd7e1/image.png)<br>\n",
    " \n",
    " * 0과 1 사이의 값을 출력<br>\n",
    " * 단점 : 그래디언트 소실 문제(Vanishing Gradient Problem) 발생<br>\n",
    "    * 학습하는 과정에서 미분을 반복하면서 발생<br>\n",
    "    * x 값이 지나치게 크거나 작을 경우, 미분값이 0에 가까워지므로 발생시켜 학습 속도를 느리게 함<br><br>\n",
    "\n",
    "4. **하이퍼볼릭 탄젠트 함수(Hyperabolic Tangent, tanh)**<br><br>\n",
    " * $\\phi(x) = 2 \\times \\phi_{sigmoid}(x) - 1 = {\\exp(x) - \\exp(-x) \\over \\exp(x) + \\exp(-x)}$<br>\n",
    " ![tanh](https://nrms.kisti.re.kr/bitextimages/TRKO201500004103/TRKO201500004103_73_image_45.jpg)<br>\n",
    " \n",
    " * 시그모이드 함수를 변형한 함수<br>\n",
    " * -1부터 1 사이의 값을 출력<br><br>\n",
    " \n",
    "5. **렐루 함수(ReLU, Rectified Linear Unit)**<br>\n",
    " * $\\phi(x) = \\begin{cases} x, & \\mbox{x }>\\mbox{0} \\\\ 0, & \\mbox{x }\\le\\mbox{0} \\end{cases}$<br>\n",
    " * $\\phi(x) = max(x, 0)$<br>\n",
    " ![relu](https://blog.kakaocdn.net/dn/cFtDSg/btq5rOn4fBs/NQpRbwc9xXAZKjkAEvYoe0/img.png)<br>\n",
    " \n",
    " * x 값이 0 이하라면 0을 출력, 양수이면 x 값을 그대로 출력<br>\n",
    " * 상한선이 없음<br><br>\n",
    " \n",
    "6. **리키 렐루(Leaky ReLU)**<br>\n",
    " * $\\phi(x) = \\begin{cases} x, & \\mbox{x }>\\mbox{0} \\\\ ax, & \\mbox{x }\\le\\mbox{0} \\end{cases}$<br>\n",
    " * $\\phi(x) = max(x, ax) (if a \\le 1)$<br>\n",
    " ![reaky](https://kh-kim.github.io/nlp_with_deep_learning_blog/assets/images/1-09/05-leaky_relu.png)<br>\n",
    " \n",
    " * 일반적으로 a = 0.01<br><br>\n",
    " \n",
    "7. **항등 함수(Identity Function)**<br>\n",
    " * $\\phi(x) = x$<br>\n",
    " ![linear](https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Function-x.svg/1200px-Function-x.svg.png)<br>\n",
    " \n",
    " * 선형 함수(Linear Function)<br>\n",
    " * 주로 회귀 문제에서의 출력층 활성화 함수로 사용<br>\n",
    " * 입력값 = 출력값<br>\n",
    " * 결괏값의 범위 제한이 없음<br>\n",
    " * x로 미분했을 때 항상 동일한 값을 가짐<br><br>\n",
    "\n",
    "8. **소프트맥스 함수(Softmax Function)**<br>\n",
    " * $\\phi(x_k) = {\\exp(x_k) \\over \\sum_{i = 1}^n \\exp(x_i)}$<br>\n",
    "    * 최종 출력층에 사용되는 함수<br>\n",
    "    * 분류 문제일 경우 소프트맥스 함수를 사용<br>\n",
    "  \n",
    " * **오버플로우(Overflow)** : 출력값이 컴퓨터가 표현할 수 있는 수의 한계를 초과하는 문제<br>\n",
    " * 오버플로우를 해결하기 위한 변형식 : $\\phi(x_k) = {\\exp(x_k \\pm C) \\over \\sum_{i = 1}^n \\exp(x_i \\pm C)}$<br>\n",
    " * 상수 C : 일반적으로 입력값의 최댓값을 이용<br>\n",
    " * 각 확률은 입력값에 비례<br><br>\n",
    " \n",
    "### 12.3.4 배치 정규화\n",
    "\n",
    "**배치 정규화(Batch Normalization)**<br>\n",
    " * 해당 층 값의 분포를 변경하는 방법<br>\n",
    " * 평균과 분산을 고정시키는 방법<br>\n",
    " * 그래디언트 소실 문제를 줄임<br>\n",
    " * 신경망의 학습 속도를 향상시킴<br>\n",
    "\n",
    "1. 전체 데이터 셋이 $x_n$이라고 했을 때, 미니 배치를 $B = {x_1, ..., x_m}$이라고 함<br><br>\n",
    "\n",
    "2. 배치 정규화의 인풋(Input)<br>\n",
    " * 미니 배치 $B = {x_1, ..., x_m}$<br>\n",
    " * 학습 대상이 되는 파라미터 $\\gamma$, $\\beta$<br><br>\n",
    " \n",
    "3. 배치 정규화를 위한 계산<br>\n",
    " * 미니 배치 평균 : $\\mu_b = {1/m}\\sum_{i = 1}^m x_i$<br>\n",
    " * 미니 배치 분산 : $\\sigma_B^2 = {1/m}\\sum_{i = 1}^m (x_i - \\mu_B)^2$<br>\n",
    " * 정규화 : $\\widehat{x}_i = {{x_i - \\mu_B} \\over \\sqrt{\\sigma_B^2 + \\epsilon}}$<br>\n",
    " * scale and shift : $y_i = \\gamma\\widehat{x}_i + \\beta$<br>\n",
    "    * 미니 배치의 평균, 분산을 이용해 정규화시켜 평균 0, 분산 1의 분포를 따르게 만듦<br>\n",
    "    * 정규화 단계에서 분모에 있는 $\\epsilon$은 상수로 분산이 0일 경우 분모가 0이 되는 경우를 방지<br>\n",
    "    * scale 파라미터 $\\gamma$와 shift 파라미터 $\\beta$를 이용해 정규화시킨 값을 아핀 변환(Affine Transformation)하면 scale과 shift가 가능<br><br>\n",
    "  \n",
    "4. 배치 정규화의 아웃풋(Output) : $y_i$<br><br>\n",
    "\n",
    "### 12.3.5 드롭아웃\n",
    "\n",
    "**드롭아웃(Dropout)**\n",
    " * 신경망의 모든 노드를 사용하는 것이 아닌, 일부 노드를 사용하지 않는 방법<br>\n",
    " ![dropout](https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F2884e4ca-d90d-4c39-bd4c-c9a95fa77258%2Fimage.png)<br>\n",
    " \n",
    " * 어떤 노드를 신경망에서 일시적으로 제거할지는 각 층에서 무작위로 선택<br>\n",
    " * 신경망의 노드 수가 줄어듦에 따라 연산량도 줄어듦<br>\n",
    " * 오버피팅 방지<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_ml",
   "language": "python",
   "name": "study_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
